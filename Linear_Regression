Linear Regression is a method for modelling the relationship between one or more indepedent variables and a dependent variable

Objective::
To find the values for the coefficient values that minimize error in prediction of the output variable

#Create dataset
from numpy import array
from matplotlib import pyplot
# define dataset
data = array([
[0.05, 0.12],
[0.18, 0.22],
[0.31, 0.35],
[0.42, 0.38],
[0.5, 0.49]])
print(data)
# split into inputs and outputs
X, y = data[:,0], data[:,1]
X = X.reshape((len(X), 1))
# scatter plot
pyplot.scatter(X, y)
pyplot.show()

2. Matrix Formulation of LR
y = X.b

# linear regression dataset
from numpy import array
from matplotlib import pyplot
# define dataset
data = array([
[0.05, 0.12],
[0.18, 0.22],
[0.31, 0.35],
[0.42, 0.38],
[0.5, 0.49]])
print(data)
# split into inputs and outputs
#code given above

3. Via Inverse
b = (X^T  X)^1  X^T  y
#using numpy
b = inv(X.T.dot(X)).dot(X.T).dot(y)

#direct solution to linear least squares
from numpy import array
from numpy.linalg import inv
from matplotlib import pyplot
# define dataset
# split into inputs and outputs
X, y = data[:,0], data[:,1]
X = X.reshape((len(X), 1))
# linear least squares
b = inv(X.T.dot(X)).dot(X.T).dot(y)
print(b)
# predict using coefficients
yhat = X.dot(b)
# plot data and predictions
pyplot.scatter(X, y)
pyplot.plot(X, yhat, color='red')
pyplot.show()

4. QR Decomposition
-In linear algebra, a QR decomposition (also called a QR factorization) of a matrix is a decomposition of a matrix A into a product 
          A = QR    Q -  orthogonal matrix  R - upper triangular matrix 
-QR decomposition is often used to solve the linear least squares problem

#using numpy
Q, R = qr(X)
b = inv(R).dot(Q.T).dot(y)

-More computationally effcient and more numerically stable than calculating the normal equation directly
-Does not work for all data matrices.

5. SVD and Pseudoinverse
 - Singular Value Decomposition
        X = U  Summationsymbol  V ^T
        
#calculate coefficients
b = pinv(X).dot(y)
print(b)

- Stable and works with most datasets.

6. Convenience Function
-NumPy provides a convenience function named lstsq() that solves the linear least squares function using the SVD approach.
# calculate coefficients
b, residuals, rank, s = lstsq(X, y)
print(b)
